# End-to-End ML Credit Default Risk Prediction Model

<img src="notebooks/Designer.jpeg" alt="logo banner" style="width: 800px;"/>

## [LIVE DEMO](https://credit-default-risk-prediction-model.streamlit.app/) -> [![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://credit-default-risk-prediction-model.streamlit.app/)

## Introduction

This project aims to develop an End-to-End machine learning model to predict loan default risk for a financial services platform. The model is trained on a dataset of borrower attributes, including financial, demographic, and credit-related features. The goal is to enable data-driven decisions in the loan underwriting process.

### Tech used

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python)![Pandas](https://img.shields.io/badge/Pandas-1.x-brightgreen?logo=pandas)![NumPy](https://img.shields.io/badge/NumPy-1.x-orange?logo=numpy)![Matplotlib](https://img.shields.io/badge/Matplotlib-3.x-blueviolet?logo=plotly)![Plotly](https://img.shields.io/badge/Plotly-5.x-informational?logo=plotly)![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-0.24%2B-lightgrey?logo=scikit-learn)![MLFlow](https://img.shields.io/badge/MLFlow-1.x-blue?logo=mlflow)![Optuna](https://img.shields.io/badge/Optuna-3.x-red?logo=optuna)![FastAPI](https://img.shields.io/badge/FastAPI-0.85%2B-brightgreen?logo=fastapi)![Streamlit](https://img.shields.io/badge/Streamlit-1.x-orange?logo=streamlit)

### Key Features

* Predictive modeling using machine learning algorithms
* Feature engineering and selection with sklearn pipelines
* Model evaluation and hyperparameter tuning with optuna
* Model registry and tracking with MLFlow
* Testing with pytest
* Deployment of API with FastAPI
* Frontend with streamlit App
* Containerization with Docker

## Installation / Environment Setup

### Prerequisites

* Python (>=3.7)
* Poetry (>=1.8) see how to install [here](https://python-poetry.org/docs/)
* Dependencies listed in `pyproject.toml` or `requirements.txt` (not to use `conda.yaml` for dependencies)

### Installation Instructions

1. Clone the repository:

   ```bash
   gh repo clone jyothisable/LoanTap-Credit-Default-Risk-Model
   ```

2. Set up virtual environment and dependencies:
  
   In a terminal, navigate to the project directory and run the following commands:
      ```bash
      poetry install
      poetry shell
      ```

## Project Structure

```ASCI
â”£ ðŸ“‚ data/ - Contains raw and processed data files for the project
â”ƒ â”£ ðŸ“„ LCDataDictionary.xlsx - Metadata dictionary for the loan dataset
â”ƒ â”£ ðŸŒ US_zip_to_cord.csv - CSV mapping U.S. ZIP codes to geographical coordinates
â”ƒ â”£ ðŸ“Š loan.csv - Main dataset containing detailed loan information (not available in GitHub because of size limitation, downloaded from [Kaggle](https://www.kaggle.com/datasets/ranadeep/credit-risk-dataset))
â”ƒ â”£ ðŸ“‰ loan_reduced.csv - Filtered and reduced version of the main dataset
â”ƒ â”£ ðŸ§ª test_data.csv - Dataset used for evaluating model performance
â”ƒ â”— ðŸ“š train_data.csv - Dataset used for training machine learning models

â”£ ðŸ§© Prediction_Model/ - Contains source code for model development, training, and predictions
â”ƒ â”£ ðŸ—ƒï¸ trained_models/ - Directory for storing trained models and pipelines
â”ƒ â”ƒ â”£ ðŸ§  XBG_model_final.pkl - Final trained XGBoost model
â”ƒ â”ƒ â”£ ðŸ“Š fe_eval_model.pkl - Feature engineering evaluation model
â”ƒ â”ƒ â”£ âš™ï¸ fe_eval_tuned_model.pkl - Tuned model for evaluating feature engineering
â”ƒ â”ƒ â”£ ðŸ”„ fe_pipeline_fitted_final.pkl - Fitted feature engineering pipeline
â”ƒ â”ƒ â”— ðŸ”™ target_pipeline_fitted.pkl - Fitted target pipeline for reverse transformations after predictions
â”ƒ â”£ ðŸ“¦ __init__.py - Initialization file for package setup
â”ƒ â”£ ðŸ§© FE_pipeline.py - Script for feature engineering pipeline configurations
â”ƒ â”£ âš™ï¸ config.py - Configuration file defining project parameters and settings
â”ƒ â”£ ðŸ§¹ data_handling.py - Script for loading, cleaning, and managing datasets and pipelines
â”ƒ â”£ ðŸ§ª evaluation.py - Script for evaluating models and feature engineering pipelines
â”ƒ â”£ ðŸ“Š get_features.py - Utility for extracting features from the data
â”ƒ â”£ ðŸ“ˆ plotting.py - Script for generating plots and visualizations
â”ƒ â”£ ðŸ¤– predict.py - Script for running predictions using trained models
â”ƒ â”— ðŸ‹ï¸ train.py - Main script for training machine learning models

â”£ ðŸ“’ notebooks/ - Directory for Jupyter notebooks, images, and analysis reports
â”ƒ â”£ ðŸ–¼ï¸ Designer.jpeg - Image file for branding or presentation purposes
â”ƒ â”£ ðŸ“„ EDA_report.html - HTML report summarizing exploratory data analysis
â”ƒ â”£ ðŸ“Š LC.png - Additional image resource related to the project
â”ƒ â”£ ðŸ·ï¸ loantap_logo.png - Logo image for the project
â”ƒ â”— ðŸ§ª model_prototyping.ipynb - Jupyter notebook for exploratory data analysis and model prototyping

â”£ ðŸ§ª tests/ - Contains unit tests and integration tests to ensure code robustness
â”ƒ â”£ ðŸ“¦ __init__.py - Initialization file for the tests package
â”ƒ â”£ ðŸ” data_tests.py - Tests for data handling and processing functions
â”ƒ â”— ðŸ§ª test_prediction.py - Tests for the prediction module

â”£ ðŸš« .dockerignore - Specifies files and directories to ignore when building Docker images
â”£ ðŸš« .gitignore - File to exclude specific files and directories from Git version control
â”£ âš™ï¸ MLProject - Configuration for running MLflow projects
â”£ ðŸš€ fastapi_app.py - Script to run a FastAPI web application for serving models or APIs
â”£ ðŸŽ¨ streamlit_app.py - Script to run a Streamlit application for visualizing data and making predictions
â”£ ðŸ‹ Dockerfile - Instructions to build a Docker container for the project
â”£ ðŸ§ª conda.yaml - Environment configuration file for setting up dependencies of MLFlow (not to be used for project setup)
â”£ ðŸ“œ requirements.txt - List of required Python packages for the project (for use with pip)
â”£ âš™ï¸ pyproject.toml - Configuration file defining project dependencies and settings using Poetry
â”£ ðŸ”’ poetry.lock - Dependency lock file for consistent environment setup using Poetry
â”£ ðŸ“œ LICENSE.md - Legal license information for the project
â”£ ðŸ“˜ README.md - Documentation file with an overview, setup, and usage instructions
```

## Usage

> [!IMPORTANT] 
> Make sure to activate poetry by running `poetry shell` in the root directory before running any commands or add  `poetry run` in beginning of every command below.

### Training

To train the model, run the following command in the root directory:

```bash
python Prediction_Model/train.py # feature engg pipeline
python Prediction_Model/train.py # Traning pipeline
```

### Prediction

To make predictions on test data, run the following command in the root directory:

```bash
python Prediction_Model/predict.py
```

### Testing

To test the model, run the following command in the root directory:

```bash
pytest tests/test_prediction.py
```

### Running Web Apps

#### fastAPI

   ```bash
   python fastapi_app.py 
   ```

   POST to `localhost:8000/predict` with Postman or use `localhost:8000/predict/docs` in browser for documentation / testing

#### Streamlit App

   ```bash
   streamlit run streamlit_app.py # local   ```
   ```

## Usage with docker

### 1. Pulling the Docker Image

To pull the Docker image from Docker Hub, run the following command:

```sh
# Pull the docker image
docker pull jyothisable/credit_risk_streamlit_app
```

### 2. Running the Docker Container

To run the Docker container, use the following command:

```sh
# Run the docker container
docker run -p 8501:8501 credit_risk_streamlit_app # goto http://localhost:8501 in browser
```

## Dataset

Refer to [here](https://www.kaggle.com/datasets/ranadeep/credit-risk-dataset) or `data/LCDataDictionary.xlsx`

## Model

The model used for this project is an XGBoost classifier. The hyperparameters used for training are as follows after tuning with optuna:

```python
{
    'max_depth': 9,
    'learning_rate': 0.094,
    'n_estimators': 507,
    'gamma': 0.0062,
    'subsample': 0.962,
    'colsample_bytree': 0.795,
    'lambda': 0.389,
    'alpha': 0.0233,
    'scale_pos_weight': 1.99,
    'min_child_weight': 2,
    'grow_policy': 'lossguide'
}
```

## Results

The model achieved an f1 score of **79.12%** and recall of **85.84%** on the test dataset.

## License

This project is licensed under the MIT License.
